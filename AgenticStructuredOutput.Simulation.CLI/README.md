````markdown
# AgenticStructuredOutput.Simulation.CLI

Command-line tool for generating evaluation test cases via the Simulation library. Wraps `LlmEvalGenerator` so you can bootstrap or augment `test-cases-eval.jsonl` directly from the shell.

## Overview

The CLI:
1. Loads the target agent prompt and schema from the solution root (or supplied paths)
2. Configures `SimulationConfig` based on CLI flags
3. Invokes the Simulation library to synthesize diverse JSON inputs
4. Persists results to a JSONL file (optionally appending to an existing dataset)

## Prerequisites

Authenticate with GitHub Models (recommended) or OpenAI:
```bash
export GITHUB_TOKEN="ghp_your_token"
# or
export OPENAI_API_KEY="sk-your-key"
```

Run commands from the CLI project directory:
```bash
cd AgenticStructuredOutput.Simulation.CLI
```

## Quick Start

Generate 10 cases and save to `generated-test-cases.jsonl` in the solution root:
```bash
dotnet run
```

## Command-Line Arguments

| Flag | Description | Default |
| --- | --- | --- |
| `--schema <path>` | Path to schema JSON | solution root `schema.json` |
| `--prompt <path>` | Prompt/agent instructions | solution root `agent-instructions.md` |
| `--output <path>` | Destination JSONL file | solution root `generated-test-cases.jsonl` |
| `--count <n>` | Number of cases to generate | `10` |
| `--diversity <0-1>` | Diversity factor passed to generator | `0.7` |
| `--temperature <0-2>` | LLM temperature | `0.8` |
| `--types <csv>` | Comma-separated evaluation types | `Relevance,Correctness,Completeness,Grounding` |
| `--no-edge-cases` | Skip explicit edge-case requests | edge cases enabled |
| `--append` | Merge with existing output file instead of overwriting | overwrite |

Examples:
```bash
# Generate 25 diverse cases and append to an existing file
dotnet run -- \
  --count 25 \
  --diversity 0.9 \
  --temperature 1.0 \
  --output ./data/test-cases-eval.jsonl \
  --append

# Target specific evaluation types
dotnet run -- --types Relevance,Grounding --count 15
```

## Output & Review

- Results are written as JSONL (`EvalTestCase` per line)
- Each case includes an expected output generated by the live agent and validated against the target schema
- When `--append` is used, previously stored cases are reloaded and merged
- Use `git diff` on the output file to review new entries before committing

## Troubleshooting

- **Authentication errors** → ensure `GITHUB_TOKEN` or `OPENAI_API_KEY` is set
- **Schema/prompt missing** → verify you are running inside the repository so defaults resolve relative to the solution root
- **Empty results** → lower diversity/temperature or reduce `--count` to confirm generation succeeds

## License

See the root `LICENSE` for details.
````
